# -*- coding: utf-8 -*-
"""doc_classification_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaG6WbSIa3YMw1rRMwFOQ2aCCY8jImR9
"""

import pandas as pd
import numpy as np
import re
#!pip install nltk
import nltk
from nltk.tokenize import word_tokenize
#!pip install pyvi
#from pyvi import ViTokenizer
nltk.download('punkt')
nltk.download('brown')
from nltk.corpus import brown
from nltk.corpus import words

def read_file(file_name):
  data_orginal = open('bbc/' + file_name)
  return data_orginal.read()


def read_all(path):
  result = []
  for i in range(1, 1000):
    try:
      file_name = ''
      if i < 10:
        file_name = path + '00' + str(i) + '.txt'
      elif i >= 10 and i < 100:
        file_name = path + '0' + str(i) + '.txt'
      else:
        file_name = path + str(i) + '.txt'
      record = read_file(file_name)
      result.append(record)
    except:
      break
  return result

documents_business = read_all(path='business/')
documents_sport = read_all(path='sport/')
documents_entertainment = read_all(path='entertainment/')
documents_politics = read_all(path='politics/')
documents_tech = read_all(path='tech/')

#print(documents_business)

data_business = pd.DataFrame(
    {
      'documents': documents_business,
      'labels' : 'business'
    }
)
data_sport = pd.DataFrame(
    {
      'documents': documents_sport,
      'labels' : 'sport'
    }
)
data_entertainment = pd.DataFrame(
    {
      'documents': documents_entertainment,
      'labels' : 'entertainment'
    }
)
data_politics = pd.DataFrame(
    {
      'documents': documents_politics,
      'labels' : 'politics'
    }
)
data_tech = pd.DataFrame(
    {
      'documents': documents_tech,
      'labels' : 'technology'
    }
)

frames = [data_business, data_sport, data_entertainment, data_politics, data_tech]
result = pd.concat(frames)
print(result)

from matplotlib import pyplot as plt
plt.style.use('fivethirtyeight')

r = result['labels'].value_counts().to_dict()

labels = list(r.keys())
numb_documents = list(r.values())
plt.barh(labels, numb_documents)

plt.xlabel('Số Documents')
plt.ylabel('Labels')
plt.title('Các loại nhãn có trong dữ liệu')

plt.tight_layout()
plt.show()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
labs = le.fit_transform(result['labels'])
#print(list(le.classes_))
genre_mappings = {index: label for index, label in 
                  enumerate(le.classes_)}
print(genre_mappings)

result['labels'] = labs

import string
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem.porter import PorterStemmer 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import word_tokenize 
lemmatizer = WordNetLemmatizer() 
stemmer = PorterStemmer() 

def text_lowercase(text): 
    return text.lower() 
  
def remove_numbers(text): 
    result = re.sub(r'\d+', '', text) 
    return result 
  
import inflect 
p = inflect.engine() 
  
# convert number into words 
def convert_number(text): 
    # split string into list of words 
    temp_str = text.split() 
    # initialise empty list 
    new_string = [] 
  
    for word in temp_str: 
        # if word is a digit, convert the digit 
        # to numbers and append into the new_string list 
        if word.isdigit(): 
            temp = p.number_to_words(word) 
            new_string.append(temp) 
  
        # append the word as it is 
        else: 
            new_string.append(word) 
  
    # join the words of new_string to form a string 
    temp_str = ' '.join(new_string) 
    return temp_str 
  
# remove punctuation 
def remove_punctuation(text): 
    translator = str.maketrans('', '', string.punctuation) 
    return text.translate(translator) 
  
# remove whitespace from text 
def remove_whitespace(text): 
    return  " ".join(text.split()) 
  
# remove stopwords function 
def remove_stopwords(text): 
    stop_words = set(stopwords.words("english")) 
    word_tokens = word_tokenize(text) 
    filtered_text = [word for word in word_tokens if word not in stop_words] 
    return " ".join(filtered_text)
  
  
# stem words in the list of tokenised words 
def stem_words(text): 
    word_tokens = word_tokenize(text) 
    stems = [stemmer.stem(word) for word in word_tokens] 
    return stems 
  
# lemmatize string 
def lemmatize_word(text): 
    word_tokens = word_tokenize(text) 
    # provide context i.e. part-of-speech 
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] 
    return " ".join(lemmas) 

def remove_tag_NNP(words_tokenize):
  wordtags = nltk.pos_tag(words_tokenize)
  words_result = []
  for wt in wordtags:
    if wt[1] != 'NNP':
      words_result.append(wt[0].lower())
  return " ".join(words_result)

def clean_text(document):
  doc_remove_tag_NNP = remove_tag_NNP(word_tokenize(document))
  doc_lower = text_lowercase(doc_remove_tag_NNP)
  doc_convertNumber = convert_number(doc_lower)
  doc_remove_punctuation = remove_punctuation(doc_convertNumber)
  doc_remove_whitespace = remove_whitespace(doc_remove_punctuation)
  doc_remove_stopwords = remove_stopwords(doc_remove_whitespace)
  doc_lematize = lemmatize_word(doc_remove_stopwords)
  return doc_lematize

text = 'Harrison uses scientific methods algorithms and many types of processes'
clean_text(text)

from sklearn.model_selection import train_test_split 
docs_train, docs_test, y_train, y_test = train_test_split(
                                                result['documents'], result['labels'],  
                                                test_size=0.2,  
                                                random_state=1000,
                                                stratify=result['labels'])

docs_train = docs_train.apply(clean_text)
docs_test = docs_test.apply(clean_text)

print(docs_train)

from sklearn.feature_extraction.text import CountVectorizer
from nltk import word_tokenize

vectorizer = CountVectorizer(min_df = 3, encoding = 'utf-8', ngram_range = (1,2))
cv_fit = vectorizer.fit_transform(docs_train)
print(vectorizer.vocabulary_)
smatrix = vectorizer.transform(docs_train)
smatrix.todense().shape

word_list = vectorizer.get_feature_names();    
count_list = cv_fit.toarray().sum(axis=0) 
print(word_list)
print(len(word_list))
print(count_list)
print(len(count_list))

from sklearn.feature_extraction.text import TfidfTransformer
tfidf = TfidfTransformer() #by default norm = "L2"
tfidf.fit(smatrix)
print("IDF:", tfidf.idf_)
print(len(tfidf.idf_))

tf_idf_matrix = tfidf.fit_transform(smatrix)
print(tf_idf_matrix.shape)

X_train = np.array(tf_idf_matrix.toarray())
X_test = np.array(tfidf.transform(vectorizer.transform(docs_test)).toarray())

# from sklearn.preprocessing import MinMaxScaler
# scaler1 = MinMaxScaler()
# X_train = scaler1.fit_transform(X_train)

# scaler2 = MinMaxScaler()
# X_test = scaler2.fit_transform(X_test)
print(X_train.shape)
print(X_test.shape)
print(type(X_train))
print(type(X_test))

def predict_model_scikitlearn(model, sentence):
  text = [clean_text(sentence)]
  x = np.array(tfidf.transform(vectorizer.transform(text)).toarray())
  y_pred = model.predict(x)
  print(le.inverse_transform(y_pred.tolist()))

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(y_train),
                                                 y_train)
#class_weights
class_weight_dict = dict(enumerate(class_weights))

from sklearn.linear_model import LogisticRegression  
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score 
model_logistic = LogisticRegression(max_iter=1000, solver = 'liblinear', multi_class = 'ovr')
model_logistic.fit(X_train, y_train)
y_pred = model_logistic.predict(X_test)
print('Accuracy: ',accuracy_score(y_test,y_pred))
print()
print(classification_report(y_test, y_pred))

from sklearn import svm
model_svm = svm.SVC(kernel = 'linear', gamma = 'auto', class_weight=class_weight_dict)
model_svm.fit(X_train, y_train)
y_pred_svm = model_svm.predict(X_test)
print('Accuracy: ',accuracy_score(y_pred_svm,y_test))
print()
print(classification_report(y_test, y_pred_svm))

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 80, class_weight=class_weight_dict)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print('Accuracy: ',accuracy_score(y_pred_rf,y_test))
print()
print(classification_report(y_test, y_pred_rf))

doc_t = """
First known 'murder hornet' nest discovered, destroyed in US
Washington State Department of Agriculture workers locate and demolish Asian giant hornet nest

A brood of periodical cicadas is set to emerge from underground for the first time in 17 years.

Brood X , also known as the Great Eastern Brood or the "Big Brood," will likely first emerge across the Southern U.S. at the beginning of spring, before surfacing in states along the East Coast.

Americans can likely expect to see the swarm across 15 states as the ground temperature warms, according to a mapping site from the  University of Connecticut , including the District of Columbia , Delaware , Maryland , Virginia , West Virginia , North Carolina , Pennsylvania , New Jersey , New York , Ohio , Michigan , Illinois , Indiana , Kentucky , Georgia , and  Tennessee .

FIRST CLONE OF US ENDANGERED SPECIES ANNOUNCED

"The reason that this is such an impressive event...is number one: it happens nowhere else on the planet. We have periodical cicadas. There are cicadas on every continent except Antarctica, but it's just in the eastern half of the United States that we have the periodical cicadas," the University of Maryland's professor emeritus Dr. Michael Raupp told Fox News on Tuesday.

"The other piece of this puzzle is that they're going emerge in massive numbers. And, when I say massive, I mean massive," he continued. "There are going to be as many as 1.5 million cicadas per acre. This is going to translate up and down the coast west of the Mississippi -- people say billions. I tend to hyperbole, but I'm saying there are probably going to be trillions."

Adult cicadas from brood X dry their wings on leaves May 16, 2004 in Reston, Virginia. - file photo. (Photo by Richard Ellis/Getty Images)

"That's a whole lot of acres in there, so there are going to be a lot of cicadas up and out of the ground in the middle of May and the end of May this year," Raupp said.

The black-bodied insects with bright red eyes last appeared en masse in 2004 , catching the eye -- and shoulder -- of golf  legend  Tiger Woods .

Much like salmon as they swim upstream, the millions of teenage cicadas have one thing on their mind: sex .

DOZENS OF EARTHQUAKES ROCK OREGON'S MOUNT HOOD

For almost two decades, a larval Brood X has survived a foot or more below the ground, living on sap from tree roots.

According to the  National Wildlife Federation , periodical cicadas will come out as adults every 13 or 17 years, unlike regular cicadas.

The brood is made up of three  different kinds of species, The Washington Post reported Tuesday, including Magicicada septendecim, Magicicada cassini and Magicicada septendecula.

Cicadas are likely best known for their loud mating calls -- which are different for each species, the Post noted, with songs reaching up to 100 decibels.

Once a cicada successfully mates, the female cicadas will deposit tens of eggs in tree branches where the larvae or "nymphs" then hatch and burrow into the soil.

After their two-to-four-week jaunt, the adult cicadas die following the intercourse.

The next generation will surface in 2038 .

There are six species of periodical cicadas in the eastern U.S., according to CNET . Brood X is one of the largest broods of 17-year cicadas.

But, not to worry: this invasion is largely harmless, and they won't "carry dogs or small children away like in 'The Wizard of Oz,'" Raupp assured.

"It's just, you know, it's been a COVID year, a year of social unrest, a year of political unrest. Hey, this is a chance to go out in your backyard and have a National Geographic special happening right there," he said. "It's going to be birth. It's going to be death. It's going to be predation. It's going to be competition. It's going to be better than an episode of 'Outlander.' There's going to be romance in the treetops when the big boy band cranks it up."

Raupp recommended that people who might be afraid of cicadas try to learn as much about them as they can.
"""

predict_model_scikitlearn(model_svm, doc_t)

import pickle
pickle.dump(tfidf, open("tfidf.pickle", "wb"))
pickle.dump(vectorizer, open("vectorizer.pickle", "wb"))
pickle.dump(le, open("le.pickle", "wb"))
pickle.dump(model_svm, open('model_svm.pkl','wb'))#save model
pickle.dump(model_logistic, open('model_lg.pkl','wb'))